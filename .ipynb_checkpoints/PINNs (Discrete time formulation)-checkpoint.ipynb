{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USNCCM15 Short Course\n",
    "# SC15-005: Machine Learning Data-Driven Discretization Theories, Modeling and Applications\n",
    "\n",
    "# Hands-on tutorial on Physics-Informed Neural Networks \n",
    "## (Part II: Discrete time formulation)\n",
    "## Instructor: Paris Perdikaris, pgp@seas.upenn.edu\n",
    "## Code repository: https://github.com/PredictiveIntelligenceLab/USNCCM15-Short-Course-Recent-Advances-in-Physics-Informed-Deep-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-informed neural networks in discrete time\n",
    "\n",
    "One potential limitation of the continuous time neural network models (see [Part I](https://github.com/PredictiveIntelligenceLab/USNCCM15-Short-Course-Recent-Advances-in-Physics-Informed-Deep-Learning) of this tutorial) considered so far stems from the need to use a large number of collocation points $N_r$ in order to enforce physics-informed constraints in the entire spatio-temporal domain. Although this poses no significant issues for problems in one or two spatial dimensions, it may introduce a severe bottleneck in higher dimensional problems, as the total number of collocation points needed to globally enforce a physics-informed constrain (i.e., in our case a partial differential equation) will increase exponentially. Although this limitation could be addressed to some extend using sparse grid or quasi Monte-Carlo sampling schemes \\cite{bungartz2004sparse,sloan1998quasi}, in the next section, we put forth a different approach that circumvents the need for collocation points by introducing a more structured neural network representation leveraging the classical Runge-Kutta time-stepping schemes \\cite{iserles2009first}.\n",
    "\n",
    "## Model specification\n",
    "To provide an overview of the formualtion, let us apply the general form of Runge-Kutta methods with $q$ stages \\cite{iserles2009first} to a general PDE equation and obtain\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "u^{n+c_i} = u^n - \\Delta t \\sum_{j=1}^q a_{ij} \\mathcal{N}[u^{n+c_j}], \\ \\ i=1,\\ldots,q,\\\\\n",
    "u^{n+1} = u^{n} - \\Delta t \\sum_{j=1}^q b_j \\mathcal{N}[u^{n+c_j}].\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Here, $u^{n+c_j}(x) = u(t^n + c_j \\Delta t, x)$ for $j=1, \\ldots, q$. This general form encapsulates both implicit and explicit time-stepping schemes, depending on the choice of the parameters $\\{a_{ij},b_j,c_j\\}$. The above equations  can be equivalently expressed as\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "u^{n} = u^n_i, \\ \\ i=1,\\ldots,q,\\\\\n",
    "u^n = u^n_{q+1},\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "u^n_i := u^{n+c_i} + \\Delta t \\sum_{j=1}^q a_{ij} \\mathcal{N}[u^{n+c_j}], \\ \\ i=1,\\ldots,q,\\\\\n",
    "u^n_{q+1} := u^{n+1} + \\Delta t \\sum_{j=1}^q b_j \\mathcal{N}[u^{n+c_j}].\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We now proceed by placing a multi-output neural network prior on\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "u^{n+c_1}(x), \\ldots, u^{n+c_q}(x), u^{n+1}(x)\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "This prior assumption results in a \\emph{physics-informed neural network} (PINN) that takes $x$ as an input and outputs\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "u^n_1(x), \\ldots, u^n_q(x), u^n_{q+1}(x)\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "## Model training\n",
    "\n",
    "To illustrate the training procedure of a discrete-time PINN, let us consider the Allen-Cahn equation with periodic boundary conditions\n",
    "\\begin{eqnarray} \\label{eq:Allen-Cahn}\n",
    "&&u_t - 0.0001 u_{xx} + 5 u^3 - 5 u = 0, \\ \\ \\ x \\in [-1,1], \\ \\ \\ t \\in [0,1],\\\\\n",
    "&&u(0, x) = x^2 \\cos(\\pi x),\\nonumber\\\\\n",
    "&&u(t,-1) = u(t,1),\\nonumber\\\\\n",
    "&&u_x(t,-1) = u_x(t,1).\\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "The Allen-Cahn equation is a well-known equation from the area of reaction-diffusion systems. It describes the process of phase separation in multi-component alloy systems, including order-disorder transitions. For the Allen-Cahn equation, the nonlinear operator is given by\n",
    "$$\n",
    "\\mathcal{N}[u^{n+c_j}] = -0.0001 u^{n+c_j}_{xx} + 5 \\left(u^{n+c_j}\\right)^3 - 5 u^{n+c_j},\n",
    "$$\n",
    "and the shared parameters of the neural networks can be learned by minimizing the sum of squared errors\n",
    "\\begin{equation}\n",
    "SSE = SSE_n + SSE_b,\n",
    "\\end{equation}\n",
    "where\n",
    "$$\n",
    "SSE_n = \\sum_{j=1}^{q+1} \\sum_{i=1}^{N_n} |u^n_j(x^{n,i}) - u^{n,i}|^2,\n",
    "$$\n",
    "and\n",
    "\\begin{eqnarray*}\n",
    "SSE_b &=& \\sum_{i=1}^q |u^{n+c_i}(-1) - u^{n+c_i}(1)|^2 + |u^{n+1}(-1) - u^{n+1}(1)|^2 \\\\\n",
    "      &+& \\sum_{i=1}^q |u_x^{n+c_i}(-1) - u_x^{n+c_i}(1)|^2 + |u_x^{n+1}(-1) - u_x^{n+1}(1)|^2.\n",
    "\\end{eqnarray*}\n",
    "Here, $\\{x^{n,i}, u^{n,i}\\}_{i=1}^{N_n}$ corresponds to the data at time-step $t^n$. In classical numerical analysis, these time-steps are usually confined to be small due to stability constraints for explicit schemes or computational complexity constrains for implicit formulations \\cite{iserles2009first}. These constraints become more severe as the total number of Runge-Kutta stages $q$ is increased, and, for most problems of practical interest, one needs to take thousands to millions of such steps until the solution is resolved up to a desired final time. In sharp contrast to classical methods, here we can employ implicit Runge-Kutta schemes with an arbitrarily large number of stages at effectively very little extra cost.To be precise, it is only the number of parameters in the last layer of the neural network that increases linearly with the total number of stages.} This enables us to take very large time steps while retaining stability and high predictive accuracy, therefore allowing us to resolve the entire spatio-temporal solution in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Implementation\n",
    "All elements defining our discrete-time neural network implementation can be summarized in the following Tensorflow class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paris/tensorflow/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import scipy.io\n",
    "from plotting import newfig, savefig\n",
    "\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllenCahn1D:\n",
    "    # Initialize the class\n",
    "    def __init__(self, x0, u0, x1, layers, dt, q):\n",
    "        \n",
    "        # Normalization constants\n",
    "        self.mu_x, self.sigma_x = x0.mean(0), x0.std(0)\n",
    "        \n",
    "        # Normalize inputs\n",
    "        x0 = (x0 - self.mu_x)/self.sigma_x\n",
    "        x1 = (x1 - self.mu_x)/self.sigma_x\n",
    "        \n",
    "        # Store data in class\n",
    "        self.x0 = x0\n",
    "        self.x1 = x1\n",
    "        self.u0 = u0\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.dt = dt\n",
    "        self.q = max(q,1)\n",
    "    \n",
    "        # Initialize NN\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        # Load IRK weights\n",
    "        tmp = np.float32(np.loadtxt('../IRK_weights/Butcher_IRK%d.txt' % (q), ndmin = 2))\n",
    "        self.IRK_weights = np.reshape(tmp[0:q**2+q], (q+1,q))\n",
    "        self.IRK_times = tmp[q**2+q:]\n",
    "        \n",
    "        # tf placeholders and graph\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        \n",
    "        self.x0_tf = tf.placeholder(tf.float32, shape=(None, self.x0.shape[1]))\n",
    "        self.x1_tf = tf.placeholder(tf.float32, shape=(None, self.x1.shape[1]))\n",
    "        self.u0_tf = tf.placeholder(tf.float32, shape=(None, self.u0.shape[1]))\n",
    "        self.dummy_x0_tf = tf.placeholder(tf.float32, shape=(None, self.q)) # dummy variable for fwd_gradients\n",
    "        self.dummy_x1_tf = tf.placeholder(tf.float32, shape=(None, self.q+1)) # dummy variable for fwd_gradients\n",
    "        \n",
    "        self.U0_pred = self.net_U0(self.x0_tf) # N x (q+1)\n",
    "        self.U1_pred, self.U1_x_pred= self.net_U1(self.x1_tf) # N1 x (q+1)\n",
    "        \n",
    "        self.loss = tf.reduce_sum(tf.square(self.u0_tf - self.U0_pred)) + \\\n",
    "                    tf.reduce_sum(tf.square(self.U1_pred[0,:] - self.U1_pred[1,:])) + \\\n",
    "                    tf.reduce_sum(tf.square(self.U1_x_pred[0,:] - self.U1_x_pred[1,:]))                     \n",
    "        \n",
    "        # Define optimizer  with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
    "                                                   1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "                \n",
    "        # Define optimizer (use L-BFGS for better accuracy)       \n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                                method = 'L-BFGS-B', \n",
    "                                                                options = {'maxiter': 50000,\n",
    "                                                                           'maxfun': 50000,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(np.float32).eps})\n",
    "        \n",
    "        # Logger\n",
    "        self.loss_log = []\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "    def initialize_NN(self, layers):        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        num_layers = len(self.layers)\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = self.weights[l]\n",
    "            b = self.biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "    \n",
    "    def fwd_gradients_0(self, U, x):        \n",
    "        g = tf.gradients(U, x, grad_ys=self.dummy_x0_tf)[0]\n",
    "        return tf.gradients(g, self.dummy_x0_tf)[0]\n",
    "    \n",
    "    def fwd_gradients_1(self, U, x):        \n",
    "        g = tf.gradients(U, x, grad_ys=self.dummy_x1_tf)[0]\n",
    "        return tf.gradients(g, self.dummy_x1_tf)[0]\n",
    "    \n",
    "    def net_U0(self, x):\n",
    "        U1 = self.forward_pass(x)\n",
    "        U = U1[:,:-1]\n",
    "        U_x = self.fwd_gradients_0(U, x)/self.sigma_x\n",
    "        U_xx = self.fwd_gradients_0(U_x, x)/self.sigma_x\n",
    "        F = 5.0*U - 5.0*U**3 + 0.0001*U_xx\n",
    "        U0 = U1 - self.dt*tf.matmul(F, self.IRK_weights.T)\n",
    "        return U0\n",
    "\n",
    "    def net_U1(self, x):\n",
    "        U1 = self.forward_pass(x)\n",
    "        U1_x = self.fwd_gradients_1(U1, x)/self.sigma_x\n",
    "        return U1, U1_x # N x (q+1)\n",
    "    \n",
    "    def callback(self, loss):\n",
    "        print('Loss:', loss)\n",
    "    \n",
    "    def train(self, nIter):\n",
    "        tf_dict = {self.x0_tf: self.x0, self.u0_tf: self.u0, self.x1_tf: self.x1,\n",
    "                   self.dummy_x0_tf: np.ones((self.x0.shape[0], self.q)),\n",
    "                   self.dummy_x1_tf: np.ones((self.x1.shape[0], self.q+1))}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for it in tqdm(range(nIter)):\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "            \n",
    "            # Print\n",
    "            if it % 50 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                self.loss_log.append(loss_value)\n",
    "#                 print('It: %d, Loss: %.3e, Time: %.2f' % \n",
    "#                       (it, loss_value, elapsed))\n",
    "                start_time = time.time()\n",
    "    \n",
    "    \n",
    "    # Trains the model by minimizing the MSE loss using L-BFGS\n",
    "    def fine_tune(self):\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.x0_tf: self.x0, self.u0_tf: self.u0, self.x1_tf: self.x1,\n",
    "                   self.dummy_x0_tf: np.ones((self.x0.shape[0], self.q)),\n",
    "                   self.dummy_x1_tf: np.ones((self.x1.shape[0], self.q+1))}\n",
    "\n",
    "        # Call SciPy's L-BFGS otpimizer\n",
    "        self.optimizer.minimize(self.sess,\n",
    "                                feed_dict = tf_dict,\n",
    "                                fetches = [self.loss],\n",
    "                                loss_callback = self.callback)\n",
    "        \n",
    "    def predict(self, x_star):\n",
    "        x_star = (x_star - self.mu_x)/self.sigma_x\n",
    "        U1_star = self.sess.run(self.U1_pred, {self.x1_tf: x_star})          \n",
    "        return U1_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example simulation\n",
    "\n",
    "Now we are ready to set up and execute our first discrete-time PINN simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of IRK stages\n",
    "q = 100\n",
    "\n",
    "# Fully-connected neural net architecture (dimensions of each layer)\n",
    "layers = [1, 200, 200, 200, 200, q+1]\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array([-1.0])\n",
    "ub = np.array([1.0])\n",
    "\n",
    "# Number of spatial points\n",
    "N = 200\n",
    "\n",
    "# Load reference solution computed with a high-order spectral method\n",
    "data = scipy.io.loadmat('../data/AC.mat')\n",
    "\n",
    "# Fetch space/time points and solution values\n",
    "t = data['tt'].flatten()[:,None] # T x 1\n",
    "x = data['x'].flatten()[:,None] # N x 1\n",
    "Exact = np.real(data['uu']).T # T x N\n",
    "\n",
    "# Determine two temporal snapshots to be used as training data\n",
    "idx_t0 = 20\n",
    "idx_t1 = 180\n",
    "dt = t[idx_t1] - t[idx_t0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data (randomly subsampled from true solution at time t0)\n",
    "noise_u0 = 0.0\n",
    "idx_x = np.random.choice(Exact.shape[1], N, replace=False) \n",
    "x0 = x[idx_x,:]\n",
    "u0 = Exact[idx_t0:idx_t0+1,idx_x].T\n",
    "u0 = u0 + noise_u0*np.std(u0)*np.random.randn(u0.shape[0], u0.shape[1])\n",
    "\n",
    "# Location of boudanry data\n",
    "x1 = np.vstack((lb,ub))\n",
    "\n",
    "# Test data\n",
    "x_star = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 21:12:05.719909 4593620416 deprecation.py:323] From /Users/paris/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0724 21:12:06.860387 4593620416 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Define discrete-time PINN model\n",
    "model = AllenCahn1D(x0, u0, x1, layers, dt, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200000/1200000 [4:45:12<00:00, 71.61it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Train model using full batch gradient descent\n",
    "model.train(1200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative L2 error: 1.660977e-02\n"
     ]
    }
   ],
   "source": [
    "# Compute prediction using the trained model\n",
    "U1_pred = model.predict(x_star)\n",
    "error = np.linalg.norm(U1_pred[:,-1] - Exact[idx_t1,:], 2)/np.linalg.norm(Exact[idx_t1,:], 2)\n",
    "print('Relative L2 error: %e' % (error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fab4a0f7a58>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "fig = plt.figure(1)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(model.loss_log, label = '$u$')\n",
    "ax.set_yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./SavedModels/AllenCahn1D.ckpt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "model.saver.save(model.sess, \"./SavedModels/AllenCahn1D.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
